import urllib2
import threading
import Queue
import urllib
import argparse
import sys

threads         = 10
target_url      = ""
wordlist_file   = ""
resume          = None
user_agent      = "Mozilla/5.0 (X11; Linux x86_64; rv:19.0) Gecko/20100101 Firefox/19.0"
extensions      = []

def build_wordlist(wordlist_file):
    # read in the word list
    fd = open(wordlist_file, "rb")
    raw_words = fd.readlines()
    fd.close()

    found_resume    = False
    words           = Queue.Queue()

    for word in raw_words:
        word = word.rstrip()

        if resume is not None:
            if found_resume:
                words.put(word)
            else:
                if word == resume:
                    found_resume = True
                    print "Resuming wordlist from %s" % resume

        else:
            words.put(word)

    return words

def dir_buster(word_queue, extensions=None):
    while not word_queue.empty():
        attempt = word_queue.get()
        attempt_list = []

        # Check to see if there is a file extension; if not
        # it's a directory path we're bruting
        if "." not in attempt:
            attempt_list.append("/%s/" % attempt)
        else:
            attempt_list.append("/%s" % attempt)

        # if we want to bruteforce extensions
        if extensions:
            for extension in extensions:
                attempt_list.append("/%s%s" % (attempt,extension))

        # Iterate over our list of attempts
        for brute in attempt_list:
            url = "%s%s" % (target_url,urllib.quote(brute))

            try:
                headers = {}
                headers["User-Agent"] = user_agent
                r = urllib2.Request(url, headers=headers)

                response = urllib2.urlopen(r)

                if len(response.read()):
                    print "[%d] => %s" % (response.code,url)

            except urllib2.URLError,e:
                if hasattr(e, 'code') and e.code != 404:
                    print "!!! %d => %s" % (e.code,url)

                pass

extensions = [".php", ".bak", ".orig", ".inc"]

def main():
    parser = argparse.ArguementParser()
    parser.add_argument("-u" , "--url", help="Target url", required=True)
    parser.add_argument("-f" , "--file", help="The wordlist used to brute force the application", required=True)
    parser.add_argument("-t" , "--threads", type=int, help="Number of threads to spwan default 10")
    parser.add_argument("-e" , "--extensions", nargs='+', help="List of extensions")
    parser.parse_args()

    global target_url = parser.url
    global wordlist_file = parser.file

    if parser.threads is not None:
        global threads = parser.threads

    word_queue = build_wordlist(wordlist_file)
    for i in range(threads):
        t = threading.Thread(target=dirbuster, args=(word_queue, extensions,))
        t.start()

if __name__ == "__main__":
    main()
